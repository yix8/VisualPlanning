<h3 align="center">
  <a href="https://arxiv.org/abs/2505.11409">
    Visual Planning: Let's Think Only with Images
  </a>
</h3>
<h5 align="center"> If you find this project interesting, please give us a star ⭐ on GitHub to support us. 🙏🙏 </h2>


<p align="center">
  <a href="https://arxiv.org/abs/2505.11409">
    <img src="https://img.shields.io/badge/Paper-arXiv-b5212f.svg?logo=arxiv" />
  </a>
  <a href="https://huggingface.co/papers/2505.11409">
    <img src="https://img.shields.io/badge/Paper-Hugging%20Face-yellow?logo=huggingface" />
  </a>
  <a href="https://opensource.org/licenses/MIT">
    <img src="https://img.shields.io/badge/LICENSE-MIT-green.svg" />
  </a>
</p>

<p align="center">
  <img src="assets/visual_planning.png" width="75%">
</p>

### About
We introduces *Visual Planning*, a new reasoning paradigm where planning is conducted entirely through sequences of images, without relying on language. Unlike traditional multimodal models that use visual input but still reason in text, our approach enables models to “think” directly in the visual domain. We propose a reinforcement learning framework, VPRL, to train large vision models for image-based planning tasks. Our method achieves significant gains over language-based baselines on spatial navigation tasks and opens a new direction for visual reasoning research.



### 📑 Citation

If you find Visual Planning useful for your research and applications, please cite using this BibTeX:

```bibtex
@misc{xu2025visualplanningletsthink,
      title={Visual Planning: Let's Think Only with Images}, 
      author={Yi Xu and Chengzu Li and Han Zhou and Xingchen Wan and Caiqi Zhang and Anna Korhonen and Ivan Vulić},
      year={2025},
      eprint={2505.11409},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2505.11409}, 
}
```